services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: ${BASE_IMAGE:-pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime}
        INSTALL_ML_DEPS: "${INSTALL_ML_DEPS:-true}"
        DOWNLOAD_MODELS: "${DOWNLOAD_MODELS:-false}"
        STABLEVITON_SHAREPOINT_URL: ${STABLEVITON_SHAREPOINT_URL}
        CONTROLNET_OPENPOSE_URL: ${CONTROLNET_OPENPOSE_URL}
        SCHP_DRIVE_URL: ${SCHP_DRIVE_URL}
        INSTANTID_ANTELOPE_URL: ${INSTANTID_ANTELOPE_URL}
        HUGGINGFACE_HUB_TOKEN: ${HUGGINGFACE_HUB_TOKEN}
    image: tryon-api
    environment:
      - TRYON_ENGINE=${TRYON_ENGINE:-stableviton}
      - TRYON_OUTPUTS_DIR=/app/api/outputs
      - TRYON_UPLOADS_DIR=/app/api/uploads
      - TRYON_MODELS_DIR=/models
      - DOWNLOAD_MODELS_ON_START=${DOWNLOAD_MODELS_ON_START:-1}
      # Persist ML caches in the mounted /models volume so weights
      # (e.g., torchvision resnet/deeplab) are not re-downloaded
      - TORCH_HOME=${TORCH_HOME:-/models/torch}
      - HF_HOME=${HF_HOME:-/models/hf}
      - HUGGINGFACE_HUB_CACHE=${HUGGINGFACE_HUB_CACHE:-/models/hf}
      - SD15_MODEL_DIR=${SD15_MODEL_DIR:-/models/sd15}
      - SD15_MODEL_ID=${SD15_MODEL_ID:-runwayml/stable-diffusion-v1-5}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - STABLEVITON_SHAREPOINT_URL
      - CONTROLNET_OPENPOSE_URL
      - SCHP_DRIVE_URL
      - INSTANTID_ANTELOPE_URL
      - HUGGINGFACE_HUB_TOKEN
      - HF_TOKEN=${HF_TOKEN:-}
      - ENABLE_INSTANTID=${ENABLE_INSTANTID:-0}
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "8008:8008"
      - "8000:8000"
    volumes:
      - ./outputs:/app/api/outputs
      - ./uploads:/app/api/uploads
      - ./models:/models
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request;print(urllib.request.urlopen('http://localhost:8008/healthz').read().decode())"]
      interval: 20s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    # Compose v2+ convenience
    gpus: all
    # For Compose v2 with NVIDIA toolkit, this can also be used:
    # gpus: all

  web:
    image: node:20-alpine
    working_dir: /app
    command: sh -c "npm install && npm run dev -- --host 0.0.0.0 --port 5173"
    volumes:
      - ./sdk/web:/app
    ports:
      - "5173:5173"
    depends_on:
      - api
