services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: ${BASE_IMAGE:-pytorch/pytorch:2.3.1-cuda12.1-cudnn8-runtime}
        INSTALL_ML_DEPS: "${INSTALL_ML_DEPS:-true}"
        DOWNLOAD_MODELS: "${DOWNLOAD_MODELS:-false}"
        STABLEVITON_SHAREPOINT_URL: ${STABLEVITON_SHAREPOINT_URL}
        CONTROLNET_OPENPOSE_URL: ${CONTROLNET_OPENPOSE_URL}
        SCHP_DRIVE_URL: ${SCHP_DRIVE_URL}
        INSTANTID_ANTELOPE_URL: ${INSTANTID_ANTELOPE_URL}
        HUGGINGFACE_HUB_TOKEN: ${HUGGINGFACE_HUB_TOKEN}
    image: tryon-api
    environment:
      - TRYON_ENGINE=${TRYON_ENGINE:-stableviton}
      - TRYON_OUTPUTS_DIR=/tmp/tryon/outputs
      - TRYON_UPLOADS_DIR=/tmp/tryon/uploads
      - TRYON_MODELS_DIR=/models
      - DOWNLOAD_MODELS_ON_START=${DOWNLOAD_MODELS_ON_START:-1}
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - STABLEVITON_SHAREPOINT_URL
      - SCHP_DRIVE_URL
      - INSTANTID_ANTELOPE_URL
      - HUGGINGFACE_HUB_TOKEN
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - "8008:8008"
    volumes:
      - ./outputs:/app/api/outputs
      - ./uploads:/app/api/uploads
      - ./models:/models
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request;print(urllib.request.urlopen('http://localhost:8008/healthz').read().decode())"]
      interval: 20s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    # Compose v2+ convenience
    gpus: all
    # For Compose v2 with NVIDIA toolkit, this can also be used:
    # gpus: all

  web:
    image: node:20-alpine
    working_dir: /app
    command: sh -c "npm install && npm run dev -- --host 0.0.0.0 --port 5173"
    volumes:
      - ./sdk/web:/app
    ports:
      - "5173:5173"
    depends_on:
      - api
